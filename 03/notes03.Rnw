%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{3}



\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
library(broman) # used for myround 
@


\begin{frame}
\frametitle{\CHAPTER. Fitting a linear model to a sample by least squares}

\vspace{-1mm}

\begin{myitemize}
\item Recall the sample version of the linear model. 
Data are \m{y_1,y_2,\dots,y_n} and on each individual \m{i} we have \m{p} explanatory variables \m{x_{i1}, x_{i2},\dots,x_{ip}}. 
\end{myitemize}

\vspace{-1mm}

\altdisplaymath{\LMi \hspace{10mm}
  y_i = b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip} + e_i \quad\mbox{for $i=1,2,\dots,n$}
}
\begin{myitemize}
\item Using summation notation, we can equivalently write
\end{myitemize}

\vspace{-1.5mm}

\altdisplaymath{
\LMii \hspace{15mm}
  y_i = \sum_{j=1}^p x_{ij} b_j + e_i \hspace{10mm} \mbox{for $i=1,2,\dots,n$}
}
\vspace{-1mm}
\begin{myitemize}
\item We can also use matrix notation. 
Define column vectors \m{\vec{y}=(y_1,y_2,\dots,y_n)}, \m{\vec{e}=(e_1,e_2,\dots,e_n)} and \m{\vec{b}=(b_1,b_2,\dots,b_p)}. 
Define the matrix of explanatory variables, \m{\mat{X}=[x_{ij}]_{n\times p}}.
%\item
In matrix notation, writing \myref{\LMi} or \myref{\LMii} is exactly the same as
\end{myitemize}
\framebox{
\altdisplaymath{
\LMiii \hspace{16mm}
 \vec{y} = \mat{X}\, \vec{b} + \vec{e}
}
\hspace{10mm}\rule[-3mm]{0mm}{9mm}
}
\begin{myitemize}
\item Matrices give a compact way to write the linear model, and also a good way to carry out the necessary computations. 
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The least squares formula}
\begin{myitemize}
\item We seek the \myemph{least squares} choice of \m{\vec{b}} that minimizes the sum of squared error, \m{\sum_{i=1}^n e_i^2}.
\item Since \m{n} is usually much bigger than \m{p}, there is usually no value of \m{\vec{b}} for which we can exactly explain the data using the explanatory matrix \m{\mat{X}}. 
\item In other words, there is no choice of \m{\vec{b}} which solves \m{\mat{X}\,\vec{b}=\vec{y}}.
\item The least squares choice of \m{\vec{b}} turns out to be

\vspace{1mm}

\framebox{
\altdisplaymath{
\LMiv \hspace{16mm} 
 \vec{b} = \big( \mat{X}^\transpose \mat{X} \big) ^{-1}\,  \mat{X}^\transpose \vec{y}
}
\hspace{10mm}\rule[-3mm]{0mm}{9mm}
}

\vspace{1mm}

\item We will check that this is the formula R uses to fit a linear model.

\item We will also gain understanding of \myref{\LMiv} by studying the \myemph{simple linear regression} model \m{y_i = b_1 x_i + b_2 + e_i} for which \m{p=2}.

\item In the simple linear regression model, \m{b_1} and \m{b_2} are called the slope and the intercept. 
In general, \m{b_1,\dots,b_p} are called the \myemph{coefficients} of the linear model.
We call \m{\vec{b}} the coefficient vector. 
\item In R, we obtain \m{\vec{b}} using the \code{coef()} function as demonstrated below.
\end{myitemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Checking the coefficient estimates from R}

\vspace{-2mm}

\begin{myitemize}
\item Consider the example from Chapter 1, where \code{L_detrended} is life expectancy for each year, after subtracting a linear trend, and \code{U_detrended} is the corresponding detrended unemployment.
\end{myitemize}
<<reconstruct_variables,echo=F>>=
L <- read.table(file="life_expectancy.txt",header=TRUE)
L_fit <- lm(Total~Year,data=L)
L_detrended <- L_fit$residuals
U <- read.table(file="unemployment.csv",sep=",",header=TRUE)
U_annual <- apply(U[,2:13],1,mean)
U_detrended <- lm(U_annual~U$Year)$residuals
L_detrended <- subset(L_detrended,L$Year %in% U$Year)
@

<<detrended_lm>>= 
lm1 <- lm(L_detrended~U_detrended)
coef(lm1)
@
\begin{myitemize}
\item Now, we can construct the \m{\mat{X}} matrix corresponding to this linear model and ask R to compute the coefficients using the formula \myref{\LMiv}.
\end{myitemize}
<<build_X>>= 
X <- cbind(U_detrended,intercept=rep(1,length(U_detrended)))

solve( t(X) %*% X ) %*% t(X) %*% L_detrended
@


\end{frame}

\begin{frame}[fragile]
\frametitle{Checking the $\mat{X}$ matrix we constructed}

\begin{myitemize}
\item The matrix calculation on the previous slide matches the coefficients produced by \code{lm()}.
\item Take some time to check that our R implementation matches the formula \myref{\LMiv}.
\item We're fairly sure we got the computation right, because we exactly matched \code{lm()}, but it is a good idea to look at the \m{\mat{X}} matrix we constructed.
\end{myitemize}

\begin{columns}[T] 
\begin{column}{0.4\textwidth}
<<>>=
head(X)
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>=
length(U_detrended)
dim(X)
@
\end{column}
\end{columns}
\end{frame}

\begin{frame}[fragile]
\frametitle{Naming the $\mat{X}$ matrix in the linear model $\vec{y}=\mat{X}\vec{b}+\vec{e}$}

\begin{myitemize}
\item ``The \m{\mat{X}} matrix'' is not a great name since we would have the same model if we had called it \m{\mat{Z}}.
\item Many names are used for \m{\mat{X}} for the many different purposes of linear models.
\item Sheather's textbook calls \m{\mat{X}} the \myemph{matrix of predictor variables} or \myemph{matrix of explanatory variables}.
\item We call \m{\mat{X}} the \myemph{design matrix} in situations where \m{x_{ij}} is the setting of adjustable variable \m{j} for the \m{i}th run of an experiment. For example, \m{y_i} could be the stregth of an alloy made up of a fraction \m{x_{ij}} of metal \m{j} for \m{j=1,\dots,p-1}. We would also want to include an intercept, \m{x_{ip}=1}.
\item \m{\mat{X}} can also be called the \myemph{matrix of covariates}.
\item Sometimes, \m{\vec{y}} is called the \myemph{dependent variable} and \m{\mat{X}} is the \myemph{matrix of independent variables}. Scientifically, an independent variable is one that can be set by the scientist. However, independence has a different technical meaning in statistics.
\end{myitemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Fitted values}

\begin{myitemize}
\item The \myemph{fitted values} are the estimates of the data based on the explanatory variables.
For our linear model, these fitted values are

\vspace{-1mm}

\mydisplaymath{ \hat y = b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip}, \hspace{10mm}\mbox{for $i=1,2,\dots,n$}.}

\vspace{-1mm}

\item The vector of least squares fitted values \m{\vec{\hat y}=(\hat y_1,\dots,\hat y_n)} is given by

\vspace{1mm}

\framebox{
\altdisplaymath{
\LMv \hspace{10mm} 
 \vec{\hat y} = \mat{X}\vec{b} = \mat{X}\big( \mat{X}^\transpose \mat{X} \big) ^{-1} \mat{X}^\transpose \vec{y}.
}
\hspace{10mm}\rule[-3mm]{0mm}{9mm}
}

\vspace{1mm}

\item It is worth checking we now understand how R produces the fitted values for predicting detrended life expectancy using unemployment:
\end{myitemize}

<<,echo=F>>=
lm1$fitted.values<- unname(lm1$fitted.values)
@

<<>>=
my_fitted_values<-X %*% solve(t(X)%*%X) %*% t(X) %*% L_detrended 
@
\begin{columns}[T] 
\begin{column}{0.4\textwidth}
<<>>=
lm1$fitted.values[1:2]
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>=
my_fitted_values[1:2]
@
\end{column}
\end{columns}
\begin{myitemize}
\item We see that the matrix calculation \myref{\LMv} exactly matches the fitted values of the \code{lm1} model that we built earlier using \code{lm()}.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Plotting the data}

\vspace{-2mm}

\begin{myitemize}
\item We have already seen plots of the life expectancy and unemployment data before. When you fit a linear model you should look at the data and the fitted values. We plot the fitted values two different ways.
\end{myitemize}
<<plot_fitted_code,echo=T,eval=F>>=
plot(L_detrended~U_detrended)  
lines(U_detrended,my_fitted_values,lty="solid",col="blue")
abline(coef(lm1),lty="dotted",col="red",lwd=2)
@
\begin{columns}[T] 
\begin{column}{0.5\textwidth}

\vspace{-3mm}

<<plot_fitted_eval,echo=F,fig.width=5,fig.height=4,out.width="2.8in",cache=F>>=
par(mai=c(1.1,1,0.4,1))
<<plot_fitted_code>>
@
\end{column}
\begin{column}{0.4\textwidth}

\vspace{5mm}

\myquestion.
Learn about the \code{abline()} and \code{lines()} functions. Explain to yourself why the solid blue line and the dotted red line coincide. 

\end{column}
\end{columns}

\end{frame}


\begin{frame}[fragile]
\frametitle{Deriving the formula for the least squares coefficient vector}

{\bf This material will not be tested in the exam. It is presented to show you an application of differentiation and to explain where the formula \myref{\LMiv} for \m{\vec{b}} comes from.}

\begin{myitemize}
\item We derive \myref{\LMiv} for the simple linear regression model \myref{\SLRi}. 

\item The \myemph{sum of squared error} is also the \myemph{sum of the squared residuals} and is known as the \myemph{residual sum of squares (RSS)}. For simple linear regression, this is

\mydisplaymath{ 
\mathrm{RSS} = \sum_{i=1}^n (y_i -  a x_i - b)^2
}

\item To find \m{a} and \m{b} minimizing RSS, we differentiate with respect to \m{a} and \m{b} and set the derivatives equal to zero.

\question. Check that
\mydisplaymath{ 
\frac{\partial}{\partial a} \mathrm{RSS} = 2a \sum_{i=1}^n x_i^2 - 2\sum_{i=1}^n x_i(y_i -  b)
}




\end{myitemize}

\end{frame}

\end{document}

\begin{frame}[fragile]
\frametitle{}

\begin{myitemize}
\item 
\end{myitemize}

\end{frame}
