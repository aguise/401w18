%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{8}

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
# library(broman) # used for myround 
par(mai=c(0.8,0.8,0.1,0.1))
@


\begin{frame}
\frametitle{\CHAPTER. Additional topics in linear modeling}

\vspace{-2mm}

\myemph{Outline}
\begin{myitemize}
\item We now have practical skills to 
\begin{enumerate}
\item \enumerateSpace
Write down linear models,
\item \enumerateSpace
Fit them in R,
\item \enumerateSpace 
Interpret the output in terms of parameter estimates, confidence intervals and hypothesis tests,
\item \enumerateSpace
Check that R is fitting the model that we intend,
\item \enumerateSpace
Check that the model we intend is appropriate for the data.
\end{enumerate}
\item These skills provide a foundation for many extensions helpful for particuluar situations.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Topics}

\begin{myitemize}
\item  The linear model formula notation in R, as a third model representation to join the subscript format and matrix format.  
\item Interactions between explanatory variables.
\item The \m{R^2} statistic to assess model fit.
\item  Fitting polynomial relationships using linear models.
\item  Multicollinearity: What happens when two or more explanatory variables are highly correlated. How to notice it, and what to do about it.
\item Power: What is the probability of rejecting the null hypothesis when the alternative is true?
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The R model formula notation}

\vspace{-2mm}

\begin{myitemize}
\item A \myemph{formula} in \code{lm()} is something that looks like \code{y~x}.
\item The R formula notation has various conventions that are designed to make it easy to specify useful models.
\item \code{?formula} tells you everything you need to know, and more.
\item The R formula for \code{lm()} is a way of constructing a design matrix.
\item Inspect the resulting design matrix using \code{model.matrix()} and check you understand what R has produced. If you can do this, you can safely use the power of the formula notation.
\end{myitemize}
\myquestion. In a report, the model should be written in mathematical notation, not as an R formula. Why?

\vspace{25mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Experimenting with the R formula notation}

\begin{myitemize}
\item Consider the freshman GPA data
<<gpa_data>>=
gpa <- read.table("gpa.txt",header=T); head(gpa,3)
@

\item We can play the game of trying out various things in R formula notation, inspecting the resulting design matrix, and figuring out how to write the model efficiently in mathematical notation.
\item You can also think about whether the different models give any new insights into the data.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
<<lm_a>>=
lm1 <- lm(GPA~ACT+High_School*Year,data=gpa) 
coef(summary(lm1))[,1:2]
@
\begin{myitemize}
\item The \code{*} here denotes inclusion of an \myemph{interaction} between \code{High_School} and \code{Year}, written in the R output as \code{High_School:Year}.
\end{myitemize}

\vspace{-1mm}

\myquestion. Conceptually, what do you think an interaction between two variables is, and why might it be needed?

\vspace{15mm}

\begin{myitemize}
\item To find out exactly what R thinks an interaction is, we can check the design matrix.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
<<lm_b>>=
head(model.matrix(lm1))
@

\vspace{-3mm}

\myquestion. Write out the sample model that R has computed in \code{lm1} using subscript notation.

\vspace{30mm}

\end{frame}


\begin{frame}[fragile]
\frametitle{Interactions and additivity}
<<>>=
lm2 <- lm(GPA~ACT+High_School+Year+High_School:Year,data=gpa)
head(model.matrix(lm2),4)
@

\begin{myitemize}
\item \code{lm2} has the same design matrix as \code{lm1}.
\item We see that, in R formula notation, \code{y~u*v} is the same as \code{y~u+v+u:v}.
\item In the model \code{y~u+v} the effects of the variables  are said to be \myemph{additive}. 
\item In a causal interpretation of an additive model, the result of changing \code{u} to \code{u2} and \code{v} to \code{v2} is the sum of the marginal effect of changing \code{u} to \code{u2} plus the marginal effect of changing \code{v} to \code{v2}.
\item The interaction term \code{u:v} breaks additivity. In this case, we can't know the consequence for the fitted value of changing \code{u} to \code{u2} unless we know the value of \code{v}. 
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The interaction between ACT and high school percentile}

\begin{myitemize}
\item We have not (yet) found any interesting effect of year. Let's drop year out of the model and look for whether there is an interaction between ACT and high school percentile for predicting freshman GPA.
\end{myitemize}
<<>>=
lm3 <- lm(GPA~ACT*High_School,data=gpa)
@
\myquestion. Write out the fitted sample linear model in subscript form, letting \m{y_i}, \m{a_i}, \m{h_i} and \m{e_i} be the freshman GPA, ACT score, high school percentile and residual error respectively for the \m{i}th student.

\vspace{30mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Interpreting a discovered interaction}

\vspace{-2mm}

<<>>=
coef(summary(lm3))[,1:2]
@

\vspace{-2mm}

\myquestion. Explain in words to the admissions director what you have found about the interaction under investigation here.

\vspace{35mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Marginal effects when there is an interaction}

\vspace{-2mm}

\begin{myitemize}
\item Notice in `lm3` that the coefficients for ACT score and high school percentile are negative. That is surprising!
\end{myitemize}

\vspace{-2mm}

<<>>=
ACT_centered <- gpa$ACT-mean(gpa$ACT)
HS_centered <- gpa$Hi - mean(gpa$Hi)
lm3b <- lm(GPA~ACT_centered*HS_centered,data=gpa)
signif(coef(summary(lm3b))[,c(1,2,4)],3)
@

\vspace{-3mm}

\myquestion. After centering the variables, the interaction effect stays the same, but the marginal effects change sign. What is happening? Why?

\vspace{30mm}

\end{frame}


\begin{frame}[fragile]
\frametitle{Quantifying the improvement in the model}

\vspace{-2mm} 

<<>>=
s3 <- summary(lm3)$sigma
lm4 <- lm(GPA~ACT+High_School,data=gpa)
s4 <- summary(lm4)$sigma
lm5 <- lm(GPA~1,data=gpa)
s5 <- summary(lm5)$sigma
cat("s3 =",s3,"; s4 =",s4,"; s5 =",s5)
@

\vspace{-2mm} 

\myquestion. Comment on both \myemph{statistical significance} and \myemph{practical significance} of the interaction between  a prediction of freshman GPA.


\vspace{30mm}


\end{frame}



\begin{frame}[fragile]
\frametitle{An interaction involving a factor}

\vspace{-2mm}

\begin{myitemize}
\item Let's go back to the football field goal data. 
\end{myitemize}

\vspace{-2mm} 

<<>>=
<<data>>=
goals <- read.table("FieldGoals2003to2006.csv",header=T,sep=",")
goals[1,c("Name","Teamt","FGt","FGtM1")]
lm6 <- lm(FGt~FGtM1*Name,data=goals)
@

\vspace{-2mm} 

\myquestion. What model do you think is being fitted here? Write it in subscript form, where \m{y_{ij}} is the field goal average for the \m{j}th year of kicker \m{i}, with \m{i=1,\dots,19} and \m{j=1,2,3,4}. Let \m{e_{ij}} be the residual error, and let \m{x_{ij}} be the previous year's average. Check your answer against the design matrix shown on the next slide.

\vspace{30mm}

\end{frame}

\begin{frame}[fragile]

<<>>=
X<-model.matrix(lm6) ; colnames(X)<-1:38 ; X[1:17,c(1:8,21:26)]
@

\end{frame}

\begin{frame}[fragile]
\myquestion. Interpret the ANOVA table below.

\vspace{25mm}

<<>>=
anova(lm6)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Collinear explanatory variables in a linear model}


\begin{myitemize}
\item Let \m{\mat{X}=[x_{ij}]_{n\times p}} be an \m{n\times p} design matrix. 
\item If there is a nonzero vector \m{\vect{\alpha}=(\alpha_1,\dots,\alpha_p)} such that \m{\mat{X}\vect{\alpha}=\vect{0}} then the columns of \m{\mat{X}} are \myemph{collinear}.
\item Here, \m{\vect{0}} is the zero vector, \m{(0,0,\dots,0)}.
\item We can write \m{\vect{x}_j=(x_{1j},x_{2j},\dots,x_{nj})} for the \m{j}th column of \m{\mat{X}}. Then,
\mydisplaymath{
\mat{X}\vect{\alpha} = \alpha_1\vect{x}_1+\alpha_2\vect{x}_2+\dots+\alpha_j\vect{x}_j.
}
We see that \m{\mat{X}\vect{\alpha}} can be thought of as a \myemph{linear combination of the columns of \m{\mat{X}}}.
\item Collinearity of explanatory variables has important consequences for fitting a linear model to data.
\item It can also be useful to notice whether the variables are close to collinear, meaning that  \m{\mat{X}\vect{\alpha}} is small but nonzero.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: an intercept with a coefficient for each factor}

\begin{myitemize}
\item Recall the mouse weight dataset. Consider a sample linear model,

\vspace{-2mm}

\mydisplaymath{
y_{ij}=\mu+\mu_j+e_{ij}.
}

\vspace{-2mm}

\item Suppose that we don't set the $\mu_1=0$ so we try to estimate both $\mu_1$ and $\mu_2$ at the same time as the intercept, $\mu$.
\item Let's work with just 3 mice in each treatment group, so \m{i=1,2,3} and \m{j=1,2}. The design matrix is therefore
<<>>=
X <- cbind(rep(1,6),rep(c(1,0),each=3),rep(c(0,1),each=3)) ; X
@
\item For \m{\vect{\alpha}=(1,-1,-1)}, we have \m{\mat{X}\vect{\alpha}=0}
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The least squares fit with collinear predictors}

\begin{myitemize}
\item Suppose that \m{\vect{b}} is a least squares coefficient vector, so that the fitted value vector \m{\vect{\hat y}=\mat{X}\vect{b}} minimizes \m{\sum_{i=1}^n \big(y_i - \hat y_i\big)^2}.
\item Suppose that \m{\mat{X}} is collinear, with \m{\mat{X}\vect{\alpha}=\vect{0}}.
\item Since
\mydisplaymath{
\mat{X}(\vect{b}+\vect{\alpha}) = \mat{X}\vect{b}+\mat{X}\vect{\alpha} = \mat{X}\vect{b}+\vect{0} = \mat{X}\vect{b},
}
we see that \m{\vect{b}+\vect{\alpha}} is also a least squares coefficient vector.
\item
\myemph{When \m{\mat{X}} is collinear, a least squares coefficent still exists, but it is not unique.}

\end{myitemize}
\myquestion. Let \m{c} be any number. Recall multiplication of a vector by a scalar: \m{c\vect{\alpha}=(c\alpha_1,\dots,c\alpha_p)}. Show that \m{\vect{b}+c \vect{\alpha}} is also a least squares fit.

\vspace{20mm}
\end{frame}

\begin{frame}[fragile]
\frametitle{Standard errors for collinear variables}

\myquestion. Any variable that is part of a collinear combination of variables has infinite standard error. Why?

\vspace{50mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{What does R do if give it collinear variables?}
<<>>=
mice <- read.table("femaleMiceWeights.csv",header=T,sep=",")
chow=rep(c(1,0),each=12)
hf=rep(c(0,1),each=12)
lm1 <- lm(Bodyweight~chow+hf,data=mice)
coef(summary(lm1))
@

\begin{myitemize}
\item R noticed that the three explanatory variables are collinear, and refused to fit the third
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
<<>>=
model.matrix(lm1)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Collinear variables and the determinant of $\mat{X}^\transpose\mat{X}$}

\begin{myitemize}
\item Recall that the variance of \m{\hat\beta} in the usual linear model is \m{\sigma^2 \big(\mat{X}^\transpose\mat{X}\big)^{-1}}.
\item Collinearity means the variance is infinite, a matrix version of dividing by zero.
\item Recall that a square matrix is invertible if its determinant is nonzero.
\item We can check that collinearity means \m{\mathrm{det}\big(\mat{X}^\transpose\mat{X}\big)=0}.
<<>>=
X <- model.matrix(lm1)
t(X)%*%X
det(t(X)%*%X)
@
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Linearly independent vectors and matrix rank}

\begin{myitemize}
\item Columns of a matrix that are not collinear are said to be \myemph{linearly independent}.
\item The \myemph{rank} of \m{\mat{X}} is the number of linearly independent columns.
\item \m{\mat{X}} has \myemph{full rank} if all the columns are linearly independent. In this case, we expect the least squares coefficient to be uniquely defined and so \m{\mat{X}^\transpose\mat{X}} has non-zero determinant and is invertible.
\item If \m{\mat{X}} does not have full rank, we can drop \myemph{linearly dependent} columns until the remaining columns are linearly independent. This is a practical approach to handling collinearity.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Example: reducing a design matrix to full rank}

\vspace{-2mm}

<<>>=
X <- model.matrix(lm1)
det(t(X)%*%X)
X2 <- X[,1:2]
det(t(X2)%*%X2)
@

\vspace{-2mm}

\begin{myitemize}
\item Dropping the third column of \code{X} has given us a full-rank design matrix.
\end{myitemize}
\myquestion. The least squares fitted values are the same using the predictor matrix \code{X2} as \code{X}. Why does dropping the last column not change the fitted values?

\vspace{20mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Almost collinear variables}

\begin{myitemize}
\item If the determinant of \m{\mat{X}^\transpose\mat{X}} is close to zero, the variance of the model-generated least squares coefficient vector becomes large.
\item This can happen when multiple explanatory variables are included in a model which all model similar things.
\end{myitemize}

\myquestion. Recall our data analysis using unemployment to explain life expectancy. What would happen if we added total employment as an additional explanatory variable? (Being unemployed is not the only alternative to being employed, since only adults currently looking for work are counted as unemployed.)

\vspace{30mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Using linear models to fit polynomial relationships}

\begin{myitemize}
\item Recall the basic linear trend model from Chapter~1 for data \m{y_1,\dots,y_n} with \m{y_i} measured at time \m{t_i},
\altdisplaymath{
\mathrm{[M1]} \quad y_i = b_0 + b_1 t_i + e_i, \quad i=1,\dots,n
}
\item What if the data have a trend that is not linear?
\item The next thing we might consider is a quadratic trend model,
\altdisplaymath{
\mathrm{[M2]} \quad y_i = b_0 + b_1 t_i + b_2 t_i^2 + e_i, \quad i=1,\dots,n
}
\item [M1] and [M2] are both linear models, with respectiv design matrices
\mydisplaymath{
\mat{X}^{[1]}=\begin{bmatrix}
1 & t_1 \\
1 & t_2 \\
\vdots & \vdots \\
1 & t_n
\end{bmatrix}
\quad\quad
\mat{X}^{[1]}=\begin{bmatrix}
1 & t_1 & t_1^2 \\
1 & t_2 & t_2^2 \\
\vdots & \vdots \\
1 & t_n & t_n^2
\end{bmatrix}
}
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The order $p$ polynomial model}

\begin{myitemize}
\item We can get additional flexibility in modeling trend by choosing a large \m{p} in the general order $p$ polynomial trend model,
\end{myitemize}
\altdisplaymath{
\mathrm{[M3]} \quad y_i = b_0 + b_1 t_i + b_2 t_i^2 + b_3 t_i^3 + \dots + b_pt_i^p + e_i, \quad i=1,\dots,n
}
\item [M1] and [M2] are both linear models, with respectiv design matrices
\mydisplaymath{
\mat{X}^{[1]}=\begin{bmatrix}
1 & t_1 \\
1 & t_2 \\
\vdots & \vdots \\
1 & t_n
\end{bmatrix}

\end{frame}

\begin{frame}[fragile]
\frametitle{}

\begin{myitemize}
\item 
\end{myitemize}

\end{frame}

\end{document}

------- This is just for copying to make new slides ---------

\end{frame}

\begin{frame}[fragile]
\frametitle{}

\begin{myitemize}
\item 
\end{myitemize}

\end{frame}

