
---
title: "Homework 6 solutions, STATS 401 W18"
output:
  html_document:
    theme: flatly
    toc: yes
csl: ecology.csl
---

\newcommand\E{\mathrm{E}}
\newcommand\SE{\mathrm{SE}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\prob{\mathrm{P}}
\newcommand\mat[1]{\mathbb{#1}}
\renewcommand\vect[1]{\boldsymbol{\mathrm{#1}}}
\newcommand\transpose{{\scriptscriptstyle \mathrm{T}}}
\usepackage{amsmath}


**Write solutions to the exercises in the first half of the homework.
For the data analysis, you do not have to report anything for questions 1--4. 
For questions 5 and 6, report your code together with a brief explanation.
Question 7 asks you to carefully write out the probability model you have used for the standard errors.
Recall that you are permitted to collaborate, or to use any internet resources, but you must list all sources that make a substantial contribution to your report.
As usual, following the syllabus, you are also requested to give some feedback in a "Please explain" statement.
**

--------------

### Exercises on variance, covariance and the normal distribution

1. Suppose $X$ and $Y$ are bivariate random variables with means $\mu_X$ and $\mu_Y$ respectively.
Use the linearity of expectation, together with the formulas 
\[
\var(X)=\E[X^2]-(\E[X])^2, \quad \cov(X,Y)=\E[XY]-\E[X]\, \E[Y], 
\]
to show that 
\[
\var(X+Y)=\var(X)+\var(Y) +2\cov(X,Y).
\]
This is similar to a calculation we did in class, but here it is a little easier since you can start from the above formulas for $\var(X)$ and $\cov(X,Y)$ rather than going all the way back to the basic definition
\[
\var(X)=\E\big[ (X- \E[X])^2\big], \quad \cov(X,Y)=\E\big[ (X-\E[X])(Y-\E[Y]) \big]. 
\]

\begin{equation} \label{Q1}
var(X+Y)=E[(X+Y)^2] - [E(X+Y)]^2
\end{equation}

Now, 
\begin{align*}
E[(X+Y)^2] &= E(X^2 + Y^2 + 2XY)\\
&= E(X^2) + E(Y^2) + 2E(XY)\\
\end{align*}

Also, 
\[ [E(X+Y)]^2 = [E(X)+E(Y)]^2 = [E(X)]^2 + [E(Y)]^2 +2 E(X)E(Y) \]

Substituting both of these in equation \ref{Q1}, we have
\begin{align*}
Var(X+Y) &= E(X^2) + E(Y^2) + 2E(XY) - [E(X)]^2 - [E(Y)]^2 - 2 E(X)E(Y) \\
&= \underbrace{E(X^2) - [E(X)]^2}_{Var(X)} + \underbrace{E(Y^2) - [E(Y)]^2}_{Var(Y)} + 
    \underbrace{2E(XY)- 2 E(X)E(Y)}_{2Cov(X,Y)}\\
&= Var(X) + Var(Y) + 2 Cov(X,Y)
\end{align*}

2. 
#### (a)
Define A such that $Y+AZ$ where $Z=\begin{bmatrix}Z_1 \\ Z_2 \\ Z_3\end{bmatrix}$.
So we have here that 
$$ Y = \begin{bmatrix}Y_1 \\ Y_2 \\ Y_3\end{bmatrix} = \begin{bmatrix}6 & 3 & 1 \\ 0 & 5 & 2 \\ 0 & 0 & 4\end{bmatrix}\begin{bmatrix}Z_1 \\ Z_2 \\ Z_3\end{bmatrix} = 
\begin{bmatrix}6Z_1 +3Z_2 + Z_3 \\ 5Z_2 + 2Z_3 \\ 4Z_3\end{bmatrix}. $$
Hence we see that $Y$ is a vector containing 3 random variables, where $Y_1 = 6Z_1 +3Z_2 + Z_3$ , $Y_2 = 5Z_2 + 2Z_3$ and $Y_3 = 4Z_3$.

We then know that Var(Y) = Var(AZ) = A Var(Z) $\text{A}^T = \text{A}\mat{I}\text{A}^T = \text{AA}^T$. 

Hence, Var(Y) = $\text{A} \text{A}^T$.
```{r}
A <- matrix(c(6,0,0,3,5,0,1,2,4),nrow=3)
VarY <- A%*%t(A)
VarY
```

#### (b)

Var($Y_1 - Y_2$) = Var($Y_1$) + Var($Y_2$) - 2Cov($Y_1$,$Y_2$). 

From the Var(Y) as obtained in the previous part, we know that Var($Y_1$) = 46, Var($Y_2$) = 29 and 2Cov($Y_1$,$Y_2$) = 2*17 = 34.

So, Var($Y_1 - Y_2$) = 46 + 29 - 34 = 41.

Doing it in matrix form, $\var(\mat{A}\vect{Y})= \mat{A}\var(\vect{Y})\mat{A}^\transpose$ where $\mat{A} = \begin{bmatrix} 1 & -1 & 0\end{bmatrix}$
```{r}
A <- matrix(c(1,-1,0),nrow=1)
VarAY <- A %*% VarY %*% t(A)
VarAY
```

#### (c)
To use pnorm, we need the mean and variance of $Y_1-Y_2$.

We know that Var($Y_1 - Y_2$) = 34

E($Y_1 - Y_2$) = E($Y_1$) - E($Y_2$).

Recall from (a) that $Y_1 = 6Z_1 +3Z_2 + Z_3$ and $Y_2 = 5Z_2 + 2Z_3$.

So, E($Y_1 - Y_2$) = E($Y_1$) - E($Y_2$) = E($6Z_1 +3Z_2 + Z_3$) - E($5Z_2 + 2Z_3$) = 6E($Z_1$) + 3E($Z_2$) + E($Z_3$) - 5E($Z_2$) -2E($Z_3$) = 0

```{r}
pnorm(3,mean = 0, sd = sqrt(34), lower.tail = FALSE )
```


3. Let $\vect{X}=(X_1,\dots,X_n)$ be a random vector with expectation $\E[\vect{X}]=\vect{\mu}$ for $\vect{\mu}=(\mu_1,\dots,\mu_n)$. 
Let $\mat{V}$ be the $n\times n$ variance matrix of $\vect{X}$, so $\mat{V}_{ij}=\cov(X_i,X_j)$ for $i=1,\dots,n$ and $j=1,\dots,n$. 
    (a) Interpreting $\vect{X}$ and $\vect{\mu}$ as column vectors, show that 
<br><br>
$[\mathrm{Eq.~1}] \hspace{1.5cm}
\mat{V}= \E[\vect{X}\vect{X}^\transpose]- \vect{\mu}\vect{\mu}^\transpose.$
<br><br>
    (b) Now let $\vect{Y}=(Y_1,\dots,Y_m)$ be a random vector with expectation $\E[\vect{Y}]=\vect{\nu}$ for $\vect{\nu}=(\nu_1,\dots,\nu_m)$. 
Let $\mat{A}=[a_{ij}]_{n\times m}$ be an arbitrary constant $n\times m$ matrix.
Apply [Eq. 1] with $\vect{X}=\mat{A}\vect{Y}$ and use the matrix linearity of expectation (i.e., summation and multiplication by a constant matrix can be moved through $\E$) to show that
<br><br>
$[\mathrm{Eq.~2}] \hspace{1.5cm}
\var(\mat{A}\vect{Y}) = \mat{A}\var(\vect{Y})\mat{A}^\transpose.$
<br><br>
You will need the formula we gave earlier in the notes for the transpose of a matrix product, $\big(\mat{A}\vect{Y}\big)^\transpose = \vect{Y}^\transpose\mat{A}^\transpose$.


Ans
(a) 
    
\[ \mat{V}_{ij}=\cov(X_i,X_j) \]
\begin{align*}
Cov(X_i,X_j) &= E(X_i,X_j) - E(X_i)E(X_j)\\
&= E(X_i,X_j) - \mu_i\mu_j \\
&= E({\mathbf{X} \mathbf{X}^T)}_{ij} - [\boldsymbol{\mu} \boldsymbol{\mu}^T]_{ij}
\end{align*}
        
Note that since $\vect{X}$ and $\vect{\mu}$ are column vectors, 
<br><br>
<br><br>

\[ \vect{X} \vect{X}^T =\begin{bmatrix}X_1 \\ X_2 \\ \vdots \\ X_n\end{bmatrix} \begin{bmatrix}X_1             & X_2 \dots X_n\end{bmatrix} = \begin{bmatrix}X_1^2 & X_1 X_2 & X_1X_3& \dots& X_1X_n \\ 
X_2X_1 & X_2^2 & X_2X_3 & \dots & X_2X_n \\ \vdots & \vdots & \vdots & & \vdots \\
X_nX_1 & X_nX_2 & X_nX_3 & \dots & X_n^2 \end{bmatrix} \]

\[ \vect{\mu} \vect{\mu}^T =\begin{bmatrix}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_n\end{bmatrix}                  \begin{bmatrix}\mu_1 & \mu_2 \dots \mu_n\end{bmatrix} = \begin{bmatrix} \mu_1^2 & \mu_1 \mu_2             & \mu_1\mu_3& \dots& \mu_1\mu_n \\ 
\mu_2\mu_1 & \mu_2^2 & \mu_2\mu_3 & \dots & \mu_2\mu_n \\ \vdots & \vdots & \vdots & & \vdots \\
\mu_n\mu_1 & \mu_n\mu_2 & \mu_n\mu_3 & \dots & \mu_n^2 \end{bmatrix} \]

Thus, $\mat{V}_{ij} =  E({\mathbf{X} \mathbf{X}^T)}_{ij} - [\boldsymbol{\mu} \boldsymbol{\mu}^T]_{ij}$ for all i,j.

So, $\mat{V}= \E[\vect{X}\vect{X}^\transpose]- \vect{\mu}\vect{\mu}^\transpose.$


#### (b)
$\var(\mat{A}\vect{Y}) = \E(\mat{A}\vect{Y} [\mat{A}\vect{Y}]^\transpose) - \mat{A}\nu [\mat{A}\nu]^\transpose$ 

$= \E( \mat{A}\vect{Y} {\vect{Y}}^\transpose {\mat{A}}^\transpose ) - \mat{A}\nu {\nu}^\transpose [\mat{A}]^\transpose$ 

$= \mat{A}\ E(\vect{Y} \vect{Y}^\transpose) \mat{A}^\transpose - \mat{A}\nu \nu^\transpose \mat{A}^\transpose$ 

$= \mat{A} \big( \E(\vect{Y} \vect{Y}^\transpose) - \nu \nu^\transpose\big) \mat{A}^\transpose$ 

$= \mat{A}\var(\vect{Y})\mat{A}^\transpose$

### Understanding how `lm()` obtains standard errors

Read the analysis of newspaper circulation data in Section 1.2.2 of Sheather. This example is continued in Section 6.2.2 of Sheather. You are now in a position to read most of this section too, but you are not required to do so at this point. 

1. Make a local copy of the data.

```{r read_data}
circulation <- read.table("circulation.txt",sep="\t",header=T)
```

2. Transform the data. Add two new columns to the dataframe called `log_Sunday` and `log_Weekday` containing the natural logarithm of the corresponding columns. The R command `log()` gives this natural logarithm, also known as log to the base $e$. We'll discuss later in class how and why to choose a suitable transformation of the data, which is an important decision for data analysis.

```{r add_log, echo=F}
circulation$log_Sunday <- log(circulation$Sunday)
circulation$log_Weekday <- log(circulation$Weekday)
```


3. Build the model in R. Create a linear model called `lm1` by fitting the logarithm of weekday circulation and the binary variable for tabloid competitor as explanatory variables for the logarithm of  Sunday circulation. Your code may look something like
```{r fit}
lm1 <- lm(log_Sunday~log_Weekday+Tabloid_with_serious_competitor,data=circulation)
```

4. Set `X` to be the design matrix using the `model.matrix()` command by typing
```{r model.matrix}
X <- model.matrix(lm1)
```

5. Use the design matrix and the response variable to compute the least squares coefficients and their standard errors by matrix calculations in R. It may be helpful to set
```{r response}
y <- circulation$log_Sunday
b <- solve(t(X) %*% X) %*% t(X) %*% y; b

#residual standard error
y_hat <- X %*% b
s <-sqrt(sum((y - y_hat)^2)/(nrow(X)-4));s

# find standard error for b
b_se <- s*sqrt(diag(solve(t(X) %*% X)));b_se
```


6. Check that these match the output of `summary(lm1)`. Also, check that your calculation of the estimated standard deviation of the measurement error matches the `residual standard error` offered by `summary(lm1)`. Why do you think `summary(lm1)` says that this is computed `on 86 degrees of freedom`?


```{r}
summary(lm1)
```
 Clearly, they do match.
 
 This is computed `on 86 degrees of freedom` because degrees of freedom in this case, n-p = 89 - 3 = 86.
 
 7. Write out in mathematical notation the probability model used to contruct these standard errors. Be careful to define the notation you use. Specify a letter for each quantity - you can use words to help define the quantities in your equation, but you should usually avoid words in an equation. 
You can write your equation either using vectors & matrices or by using subscripts to denote each unit $i$ and specifying the range of values of $i$ for which the equation holds. 
Be explicit about what quantities are random vectors. 
If you define a measurement error model, be sure to specify all means, variances and covariances for the error random variables. 

Let $Y_i$ be the Sunday ciculation of newspaper $i$.
The design matrix $\mat{X} = \begin{bmatrix}X_1 & X_2 & X_3\end{bmatrix} = \begin{bmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ \vdots \\ x_{i1} & x_{i2} & x_{i3} \\ \vdots \\ x_{n1} & x_{n2} & x_{n3} \end{bmatrix} = \begin{bmatrix} 1 & x_{12} & x_{13} \\ 1 & x_{22} & x_{23} \\ \vdots \\ 1 & x_{i2} & x_{i3} \\ \vdots \\ 1 & x_{n2} & x_{n3} \end{bmatrix}$
where $x_{i1} = 1$ (which corresponds to the intercept), $x_{i2}$ is the weekday circulation of newspaper i and $x_{i3}$ is the indicator function which takes value 1 if there is a competing tabloid, and 0 otherwise.


The model in matrix form is

$$ \vect{Y} = \mat{X} \vect{\beta} + \vect{\epsilon} $$

where $\vect{Y} = \begin{bmatrix}Y_1 \\ Y_2 \\ \vdots \\ Y_n\end{bmatrix}$ is the vector of random variables $Y_i$ which measures the Sunday circulation of newspaper i.

$\mat{X}$ is the design matrix (as defined above). We obtain this purely from the data.

$\vect{\beta} = \begin{bmatrix}  \beta_1 \\ \beta_2 \\ \beta_3\end{bmatrix}$ is the vector containing the true parameter values. 

$\vect{\epsilon} = \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n\end{bmatrix}$ is the vetor of random variables $\epsilon_i$ which measures the error corresponding to newspaper i.

Model in subscript form, for i=1,2,...,n

$$ Y_i = \beta_1 + x_{i2}\beta_2 + x_{i3}\beta_3 + \epsilon_i$$

where $\var(\epsilon_i) = \sigma^2$, $\cov(\epsilon_i,\epsilon_j)=0$ for $i\neq j$.

We model our data $\mathbf{y}= \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}$ as a draw from this probability model.



